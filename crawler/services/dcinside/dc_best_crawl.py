from selenium import webdriver
from bs4 import BeautifulSoup
from core.pyselenium import Pyselenium
import yaml
import os
from datetime import datetime, timedelta
import psycopg2
from utils.logger import get_logger

class DcinsideBest(Pyselenium):
    def __init__(self, config_f="dc_best_crawl.yaml"):
        super().__init__()
        self.driver = webdriver.Chrome()

        # í˜„ì¬ ì‹¤í–‰ íŒŒì¼ í´ë” ê¸°ì¤€ìœ¼ë¡œ config íŒŒì¼ ì§€ì •
        current_dir = os.path.dirname(os.path.abspath(__file__))
        config_path = os.path.join(current_dir, config_f)

        # ì„¤ì • config ë¡œë“œ
        with open(config_path, 'r', encoding="utf-8") as f:
            cfg = yaml.safe_load(f)

        # DB ì—°ê²°
        self.table_name = "total_articles"

        self.conn = psycopg2.connect(
            host= "localhost",
            database='postgres',
            user= "postgres",
            password= "9724",
            port= 5432
        )

        self.base_url = cfg['base_url']
        self.headers = {"User-Agent": cfg.get("user_agent")}
        self.wait_time = cfg['wait_time']
        self.is_done = False

        self.site_name = 'dcinside'
        self.repeat_article = 0

        # ë¡œê±° ì„¤ì •
        self.logger = get_logger(self.site_name)

    ###############################################################################################
    def crawl_list(self):
        articles_list = []
        self.repeat_article = 0
        # DBì— ìˆëŠ” ê²Œì‹œë¬¼ë“¤ì˜ article_id ë¶ˆëŸ¬ì˜¤ê¸°
        repeat_article = self.load_db_articles()

        html_source = self.driver.page_source
        soup = BeautifulSoup(html_source, 'html.parser')

        article_list = soup.find('tbody', class_= "listwrap2")
        contents_e =  article_list.find_all('tr', class_= "ub-content us-post thum")
        self.logger.info(f"ê²Œì‹œê¸€ ìˆ˜: {len(contents_e)}ê°œ")

        # ê²Œì‹œë¬¼ ë‚´ìš© ì¶”ì¶œ
        for e in contents_e:
            # ì œëª© ì¶”ì¶œ
            title_e = e.find('td', class_ = "gall_tit ub-word").find('a')
            strong_e = title_e.find('strong')
            if strong_e:
                strong_e.decompose()
            title = title_e.text.strip()
            # url ì¶”ì¶œ
            base_url = 'https://gall.dcinside.com'
            a_url = title_e['href']
            url = base_url + a_url
            # ì‹œê°„ ì¶”ì¶œ
            time_e = e.find('td', class_="gall_date")
            create_ts = time_e['title']
            # ê³ ìœ  ë²ˆí˜¸ ì¶”ì¶œ
            article_id = self.site_name + "_" + url.split('no=')[1].split('&')[0]

            if article_id in repeat_article:
                self.repeat_article += 1
                continue
            # ë¦¬ìŠ¤íŠ¸ ì¶”ê°€
            articles_list.append({
                'article_id': article_id,
                'article_url': url,
                'title': title,
                'create_ts': create_ts
            })
        self.logger.info(f"ì¤‘ë³µ ê²Œì‹œê¸€: {self.repeat_article}ê°œ")
        # PostgreSQL DBì— ë°ì´í„° ì‚½ì…
        try:
            with self.conn.cursor() as cursor:
                # ë°ì´í„° ì‚½ì… ì¿¼ë¦¬
                insert_query = f'''
                        INSERT INTO {self.table_name}
                        (article_id, title,  creation_date, article_url, site_name, collected_date)
                        VALUES (%s, %s, %s, %s, %s, %s)
                        ON CONFLICT (article_id, site_name) DO NOTHING;
                    '''
                # í•œë²ˆì— ë§ì€ ì–‘ì˜ ë°ì´í„° ì‚½ì…
                cursor.executemany(insert_query, [
                    (a['article_id'], a['title'], a['create_ts'], a['article_url'], self.site_name, datetime.now())
                    for a in articles_list
                ])

                self.conn.commit()
                self.logger.info(f"{len(articles_list)}ê°œ ìˆ˜ì§‘ ì™„ë£Œ ë° DB ì €ì¥ ì™„ë£Œ")
        except Exception as e:
            self.logger.error(f"DB ì €ì¥ ì‹¤íŒ¨:{e}")
            return

        return

    ###############################################################################################
    def load_db_articles(self):
        with self.conn.cursor() as cursor:
            cursor.execute(f"SELECT article_id FROM {self.table_name} WHERE site_name=%s", (self.site_name,))
            existing_ids = {row[0] for row in cursor.fetchall()}
        return existing_ids

    ###############################################################################################
    def start(self):
        try:
            self.logger.info(f"ğŸš€ {self.site_name} ì¸ê¸°ê¸€ í¬ë¡¤ë§ ì‹œì‘")
            for i in range(1, 5):
                url = f"{self.base_url}{i}"
                self.driver.get(url)
                self.logger.info(f"ìˆ˜ì§‘ URL: {url}")
                self.crawl_list()

        except Exception as e:
            self.logger.error(f"Error: {e}")
        finally:
            # ë””ë²„ê¹… ì¤‘ ê°•ì œ ì¢…ë£Œë‚˜ ì˜ˆì™¸ê°€ ë‚˜ë„ Chrome ì¢…ë£Œ
            if self.driver:
                self.driver.quit()
            if self.conn:
                self.conn.close()

if __name__ == "__main__":
    crawler = DcinsideBest()
    crawler.start()
